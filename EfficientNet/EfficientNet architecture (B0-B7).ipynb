{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbef058b",
   "metadata": {},
   "source": [
    "# EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7face627",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers, and recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline CNN by a factor of four. The conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. What if, instead, we could find a more principled method to scale up a CNN to obtain better accuracy and efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a2fc1",
   "metadata": {},
   "source": [
    "In our ICML 2019 paper, **“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”**,  a novel model scaling method that uses a simple yet highly effective compound coefficient to scale up CNNs in a more structured manner was proposed. Unlike conventional approaches that arbitrarily scale network dimensions, such as width, depth and resolution, the new method uniformly scales each dimension with a fixed set of scaling coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eafc52",
   "metadata": {},
   "source": [
    "#### Compound Model Scaling: A Better Way to Scale Up CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6626c8",
   "metadata": {},
   "source": [
    "In order to understand the effect of scaling the network, we systematically studied the impact of scaling different dimensions of the model. While scaling individual dimensions improves model performance, we observed that balancing all dimensions of the network—width, depth, and image resolution—against the available resources would best improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc4f32",
   "metadata": {},
   "source": [
    "The first step in the compound scaling method is to perform a grid search to find the relationship between different scaling dimensions of the baseline network under a fixed resource constraint (e.g., 2x more FLOPS).This determines the appropriate scaling coefficient for each of the dimensions mentioned above. We then apply those coefficients to scale up the baseline network to the desired target model size or computational budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438df5",
   "metadata": {},
   "source": [
    "**Compound Model Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e04be",
   "metadata": {},
   "source": [
    "![CompoundModelScaling](images/comparision_of_scaling_methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c79982",
   "metadata": {},
   "source": [
    "This compound scaling method consistently improves model accuracy and efficiency for scaling up existing models such as **MobileNet (+1.4% imagenet accuracy)**, and **ResNet (+0.7%)**, compared to conventional scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9bfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edb833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffd7e356",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9fd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49002a93",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051f7ff",
   "metadata": {},
   "source": [
    "The baseline information specified in the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e041c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model will be a list of list for all the baseline information\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6faf54f",
   "metadata": {},
   "source": [
    "### Phi Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2acee6",
   "metadata": {},
   "source": [
    "All the Phi values are in accordance to the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "# Gamma : resolution\n",
    "# aplha : depth\n",
    "# beta : width "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcf8d6",
   "metadata": {},
   "source": [
    "### CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ec54b",
   "metadata": {},
   "source": [
    "If we set **group=1** as we did by default then it is a **Normal conv**<br>\n",
    "if we set it to **groups = in _channels**, then it is a **Depthwise conv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78fd85",
   "metadata": {},
   "source": [
    "### Squeeze Excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0902b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8058b",
   "metadata": {},
   "source": [
    "### Inverted Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e136ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation, we will reduce it by 1/4th for whatever comes in\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        # if we use downsample or downsample the skip connection wont work as the dimensionality is different ie the height nad width wont match\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim, # depthwise conv as groups = in _channels\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        \n",
    "    def stochastic_depth(self, x): # skipping some layers during training\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor # Div to maintian the std dev and mean of the batch\n",
    "        # This comes directly from the Stochastic Depth paper\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3ef85",
   "metadata": {},
   "source": [
    "### EfficientNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "        \n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate    \n",
    "    \n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*ceil(int(channels*width_factor) / 4) # to make sure it is always divisible by 4\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1, # we will downsample at the first layer\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                        # to do same conv\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1)) # to flatter it for the linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56674d",
   "metadata": {},
   "source": [
    "### Testing on a small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4c9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    version = \"b0\"\n",
    "    phi, res, drop_rate = phi_values[version]\n",
    "    num_examples, num_classes = 4, 10\n",
    "    x = torch.randn((num_examples, 3, res, res)).to(device)\n",
    "    model = EfficientNet(\n",
    "        version=version,\n",
    "        num_classes=num_classes,\n",
    "    ).to(device)\n",
    "\n",
    "    print(model(x).shape) # (num_examples, num_classes)\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfebfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
